# Introduction to Monitoring in Google Cloud
### Operations monitoring
- Flow of monitoring
    - Capture signals = metrics, logs, traces
    - Visualize and Analyze = logs explorer, dashboards
    - Manage Incidents = alerts, error report, SLOs
    - Troubleshoot the issue
- Monitoring Resources
    - BigQuery, CloudRun, Applications, Compute send signals with data
        - these services have automatic logging
    - the signals and data get sent to monitoring tools like ObservabilitSuite
- Logging is about collecting, analyzing, exporting, retaining the logs
    - **Cloud audit logs** track admin activity and who or what (users or system) uses GCP resources
    - **Agent Logs** track sys software and third party apps
    - **Network Logs** track firewalls, network flow and security, load balancers
- Error reporting = tracks the errors and who it affects and how severe
- Service Monitoring = understand and troubleshoot intra-servce dependencies, know when SLOs levels are not met
### Application performance management
- Debugger - debug apps in real-time during production
    - can collaborate debug sessions
    - debug snapshots (app state at a certain time)
    - integrated with IDEs, Git
- Trace - tracks latency data for apps in near real-time
    - used to find performance degradations
    - detects issues automatically
- Profiler - profiles CPU and the heap. 
    - improves performance by seeing what processes consume most or too many resources
- **NOTE** Logging service is important for analyzing data, too. Not just collecting
# Avoiding Customer Pain
- **Site Reliability Engineering SRE**:
    - monitoring disply real-time data from a system
    - great products need to be deployed into great enviroments
    - plan resources ahead of time
    - need to test new versions and updates with continuous integration and dev.
    - analyze root cause or problems and then have
        - need to be transparent of when things go wrong
    - all these actions rely on monitoring. improve experience of clients and customer pain
- Why monitor?
    - continually improve product.
        - good for business analysts for bettering product, security
    - create dashboards to view logs and data
        - combined with alerts
    - alert when things go wrong
        - notify a human when system needs attention
    - be able to debug where the errors occur
        - **triggers** are system failures that send a signal and an alert
        - there is a response where the failure is analyzed and there is an **initial response** to the issue (notify customer)
        - then after the issue is resolved there is a **postmortem**. contain future documentation that analyze the issue and ways to prevent it
- When setting expectations for monitoring use KISS (Keep it simple, dumbass)
    - focus on a system one at a time and from multiple perspectives
    - "use single pane of glass" rule. view several dashboards of projects. not all of the projects available but group them reasonably
    - monitoring systems should address (what vs. why)
        - what? = symptom or error indicator (what is broken)
        - why? = cause or reason for what is causing the error
- White vs. black box:
    - black box = testing public behaviors such as that of a consumer
    - white box = monitoring based on internal logs and systems
- There are business metrics and technical metrics
    - how to define a metric?? use SMART [Specifc, Measurable, Achievable, Relevant, Time bound (should be a rate)]
    - can start with a metric using the **Four Golden Signals**
        - **Latency**: how long for a system to return a result. Ex:
            - num. req. wating for a thread
            - time until first resp.
        - **Traffic**: how many requests hitting your system. Ex:
            - num. HTTP req. placed per sec.
            - num. active connections
        - **Saturation**: how full a service is on capacity of resources
            - % CPU usage, etc.
        - **Errors**: system failures are measured. might indicate an SLO violation and need to make a customer response
            - Wrong answers/content
            - num. 400/500 HTTP codes
- **Service Level Indicator** = quantifiable measure of service's reliability. Good events = Valid events
- **Service Level Objective** = target of reliability. needed by services
    - availability = measure service to run when needed
    - reliability = measure services ability to perform its intended function
        - hard for developers to work with operators (devops) to improve reliability
    - find the optimal level of availability/reliability and level of happiness such that the happiness is barely met
        - "barely met" bc no need to expend more resources for better performance. 
    - should include some room for error
    - should have shorter time windows (around 28 days) for evaluating performance
    - error budgets are usually burned by feature releases, system changes, or failure in hardware, networks
        - can also define downtime in the SLA
- **Service Level Agreement** = minimal level of service before breaking the agreement and then paying them for building a shitty product
    - SLO target should be higher than the SLAs or else you pay customers
- **Customers** = subset of users who pay for service
- How to choose a good SLI?
    - the metric should measure the performance that the consumer is getting
        - Response time, data processing, Storage
    - metrics should also also be visible and with less noise
        - this helps with defining good and bad events
    - want a SLI that correlates with happiness
        - SLI = good events / valid events (it's a ratio)
            - valid events should not include the error events bc it will ruin budgeting
    - should have aroudn 3-5 SLIs for each user journey
    - what are the users expectations?
    - how does a user interact with the service?
- SLO also represent the line where users are happy or not
    - too reliable SLOs means it risks breaking agreements
    - not too reliable SLOs means that it costs more to keep customers
- set SLOs and reliability targets early and base it off previous data as possible
- Each user journey:
    - define an SLI and then refine them
        - Response time, data processing, Storage
        - (refine) be clear what is being measured and how we define a sucessful event
    - base them off previous performance and business data
        - choose cut off point from historical data
- SLI must capture multiple journeys, consider edge cases, analyzed for cost benefit
- **NOTE:** - The error budgets are calculated as 100% minus the SLOs (error budget is very small)
    - The error budget is not close to 100%
    - New features can be developed as long as its within the error budget
    - There should be alerts if a service is consuming a large amount of the error budget
    - Four golden signals = latency, traffic, saturation, errrors
# Alerting Policies
- Alerting = generating when things need to change due to danger or issues
    - SLO = achievable target, SLI = what is measured
    - alerts are timebased and summarized in a period
    - **Detection time** - how long for system to notice issue and then fire the alert
    - **Reset time** - how long until another alert can be fired after the issue is fixed
    - **Precision** - relevant alerts / total alerts
    - **Recall** - Relevent alerts / relevant alerts + the alerts missed
- Window Length - smaller windows allow for faster alert detection
    - the window is the time period where errors get accounted
        - if the errors pass the error budget then the SLO is broken
    - longer windows allow for more precision. 
- **Duration** = something added for better precision
    - errors spotted are treated as an anomally until a duration
        - downside is that errors that occur within the duration get ignored (cost more then)
- Use multiple confiditons for better Precision and Recall
- prioritize alerts based on Customer Impact
- **Alert Policies** - conditions are super important for an alert
    - what are aligners?
        - help define window/time period to compare metrics
    - can use multiple conditions
    - notifcation channels determine how the alert is sent (email, sms, slack)
- Can add alerts to uptime checks and log-based metrics
    - also attach alerts to a group of GCP resources
        - can add resources to groups based on criterion    
        - will search all resources and add based on criteria
- Make alerts using the Console, Shell, or API (yaml file, too)
## Lab notes
```bash

```
- Service monitoring helps with maintaining SLOs
    - tracks error budgets too
- Error Budget Details:
    - 1 error every 1000 requests, thus reliability is 99.9%
    - can set SLA to be 5 errors every 1000 requests thus error budget is 5 errors
- window-based vs request-based SLOs
    - window-based hides burst related errors (not fun for customer)
- can make alerts when SLO is burning faster than expected
## Lab notes
```bash
```
- **NOTE:** alerting policies: precision = proportion of events detected (doesn't have to be accurate) that were signifcant
    - signficant means that we wanted that alert. whereas the all events detected could include events we didn't want to be alerted
    - another way to evaluate the alert policy is to use recall = proportion of alerts detected that were signficant to the amount of alerts that were significant plus the alerts that didn't fire (called missed alerts)
# Monitoring Cricial Systems
- Monitor workspace so you have a single-pane of glass
    - view multiple project status and metrics and data
    - can view AWS projects as well
    - each project can have only one workspace for monitoring
    - smaller workspaces so people has less access to too many projects
- some services need service account to write metrics to the Monitoring workspace
- Google auto adds charts and dashboards for your resources in the Project
    - GKE, Compute Engine, App Engine
    - there are metric types, metric data type and other descriptors used by dashboards and collected by Google Monitoring
- can filter data on charts (remove noise and focus on data with specific criteria)
    - group data and then combine them in charts
    - can choose the design of a chart that give off some information of data
- **Aligners** = break data points into time buckets (alignment period)
- Dashboards are configurable
    - can view many charts and export the dashboards
- **Uptime checks** = finds issues at protocols, hosts, and ports
    - understands if a response is a failure or success
- Error budget = allow for some errors to occur
    - this means that errors are a currency and can experiement with code
- **NOTE:** - Uptime check is good to monitor applications then get notified if they are down
    - Health checks are good to monitor VMs from Compute Engine
    - Monitoring dashboards require the Monitoring Viewer role